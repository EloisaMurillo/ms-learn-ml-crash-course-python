{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "colab": {
      "name": "11. Recurrent Neural Networks - Python.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EloisaMurillo/ms-learn-ml-crash-course-python/blob/master/Ej%2011.%20Recurrent%20Neural%20Networks%20-%20Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Jzh1FBUrOjpX"
      },
      "source": [
        "Exercise 11 - Recurrent Neural Networks\n",
        "========\n",
        "\n",
        "A recurrent neural network (RNN) is a class of neural network that excels when your data can be treated as a sequence - such as text, music, speech recognition, connected handwriting, or data over a time period. \n",
        "\n",
        "RNN's can analyse or predict a word based on the previous words in a sentence - they allow a connection between previous information and current information.\n",
        "\n",
        "This exercise looks at implementing a LSTM RNN to generate new characters after learning from a large sample of text. LSTMs are a special type of RNN which dramatically improves the model’s ability to connect previous data to current data where there is a long gap.\n",
        "\n",
        "We will train an RNN model using a novel written by H. G. Wells - The Time Machine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_sxLemUOjpY"
      },
      "source": [
        "Step 1\n",
        "------\n",
        "\n",
        "Let's start by loading our libraries and text file. This might take a few minutes.\n",
        "\n",
        "#### Run the cell below to import the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "gYBDzfCfOjpZ"
      },
      "source": [
        "%%capture\n",
        "# Run this!\n",
        "from keras.models import load_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, LSTM\n",
        "from keras.callbacks import LambdaCallback, ModelCheckpoint\n",
        "import numpy as np\n",
        "import random, sys, io, string"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6IaKNOROjpe"
      },
      "source": [
        "#### Replace the `<addFileName>` with `The Time Machine`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "y-z1a9lFOjpf",
        "outputId": "53812de2-24a9-4d6b-edf9-ad30f27012a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "###\n",
        "# REPLACE THE <addFileName> BELOW WITH The Time Machine\n",
        "###\n",
        "text = io.open('/content/The Time Machine.txt', encoding = 'UTF-8').read()\n",
        "###\n",
        "\n",
        "# Let's have a look at some of the text\n",
        "print(text[0:198])\n",
        "\n",
        "# This cuts out punctuation and make all the characters lower case\n",
        "text = text.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "# Character index dictionary\n",
        "charset = sorted(list(set(text)))\n",
        "index_from_char = dict((c, i) for i, c in enumerate(charset))\n",
        "char_from_index = dict((i, c) for i, c in enumerate(charset))\n",
        "\n",
        "print('text length: %s characters' %len(text))\n",
        "print('unique characters: %s' %len(charset))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿The Time Traveller (for so it will be convenient to speak of him) was expounding a recondite matter to us. His pale grey eyes shone and twinkled, and his usually pale face was flushed and animated.\n",
            "text length: 174201 characters\n",
            "unique characters: 39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecbIhj68Ojpi"
      },
      "source": [
        "Expected output:  \n",
        "```The Time Traveller (for so it will be convenient to speak of him) was expounding a recondite matter to us. His pale grey eyes shone and twinkled, and his usually pale face was flushed and animated.\n",
        "text length: 174201 characters\n",
        "unique characters: 39```\n",
        "\n",
        "Step 2\n",
        "-----\n",
        "\n",
        "Next we'll divide the text into sequences of 40 characters.\n",
        "\n",
        "Then for each sequence we'll make a training set - the following character will be the correct output for the test set.\n",
        "\n",
        "### In the cell below replace:\n",
        "#### 1. `<sequenceLength>` with `40`\n",
        "#### 2. `<step>` with `4`\n",
        "#### and then __run the code__. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "ey8N2lsHOjpj",
        "outputId": "4b30fe28-fdb9-4155-e81f-0ef7037e73aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "###\n",
        "# REPLACE <sequenceLength> WITH 40 AND <step> WITH 4\n",
        "###\n",
        "sequence_length = 40\n",
        "step = 4\n",
        "###\n",
        "\n",
        "sequences = []\n",
        "target_chars = []\n",
        "for i in range(0, len(text) - sequence_length, step):\n",
        "    sequences.append([text[i: i + sequence_length]])\n",
        "    target_chars.append(text[i + sequence_length])\n",
        "print('number of training sequences:', len(sequences))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of training sequences: 43541\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdP7dwQCOjpm"
      },
      "source": [
        "Expected output:\n",
        "`number of training sequences: 43541`\n",
        "\n",
        "#### Replace `<addSequences>` with `sequences` and run the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "GyAICIj4Ojpm"
      },
      "source": [
        "# One-hot vectorise\n",
        "\n",
        "X = np.zeros((len(sequences), sequence_length, len(charset)), dtype=np.bool)\n",
        "y = np.zeros((len(sequences), len(charset)), dtype=np.bool)\n",
        "\n",
        "###\n",
        "# REPLACE THE <addSequences> BELOW WITH sequences\n",
        "###\n",
        "for n, sequence in enumerate(sequences):\n",
        "###\n",
        "    for m, character in enumerate(list(sequence[0])):\n",
        "        X[n, m, index_from_char[character]] = 1\n",
        "    y[n, index_from_char[target_chars[n]]] = 1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMjWKPDNOjpq"
      },
      "source": [
        "Step 3\n",
        "------\n",
        "\n",
        "Let's build our model, using a single LSTM layer of 128 units. We'll keep the model simple for now, so that training does not take too long.\n",
        "\n",
        "### In the cell below replace:\n",
        "#### 1. `<addLSTM>` with `LSTM`\n",
        "#### 2. `<addLayerSize>` with `128`\n",
        "#### 3. `<addSoftmaxFunction>` with `'softmax`\n",
        "#### and then __run the code__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "3fA136vcOjpr"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "###\n",
        "# REPLACE THE <addLSTM> BELOW WITH LSTM (use uppercase) AND <addLayerSize> WITH 128\n",
        "###\n",
        "model.add(LSTM(128, input_shape = (X.shape[1], X.shape[2])))\n",
        "###\n",
        "\n",
        "###\n",
        "# REPLACE THE <addSoftmaxFunction> with 'softmax' (INCLUDING THE QUOTES)\n",
        "###\n",
        "model.add(Dense(y.shape[1], activation = 'softmax'))\n",
        "###\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qE3tH5kOjpv"
      },
      "source": [
        "The code below generates text at the end of an epoch (one training cycle). This allows us to see how the model is performing as it trains. If you're making a large neural network with a long training time it's useful to check in on the model as see if the text generating is legible as it trains, as overtraining may occur and the output of the model turn to nonsense.\n",
        "\n",
        "The code below will also save a model if it is the best performing model, so we can use it later.\n",
        "\n",
        "#### Run the code below, but don't change it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "0d_aSMKBOjpv"
      },
      "source": [
        "# Run this, but do not edit.\n",
        "# It helps generate the text and save the model epochs.\n",
        "\n",
        "# Generate new text\n",
        "def on_epoch_end(epoch, _):\n",
        "    diversity = 0.5\n",
        "    print('\\n### Generating text with diversity %0.2f' %(diversity))\n",
        "\n",
        "    start = random.randint(0, len(text) - sequence_length - 1)\n",
        "    seed = text[start: start + sequence_length]\n",
        "    print('### Generating with seed: \"%s\"' %seed[:40])\n",
        "\n",
        "    output = seed[:40].lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    print(output, end = '')\n",
        "\n",
        "    for i in range(500):\n",
        "        x_pred = np.zeros((1, sequence_length, len(charset)))\n",
        "        for t, char in enumerate(output):\n",
        "            x_pred[0, t, index_from_char[char]] = 1.\n",
        "\n",
        "        predictions = model.predict(x_pred, verbose=0)[0]\n",
        "        exp_preds = np.exp(np.log(np.asarray(predictions).astype('float64')) / diversity)\n",
        "        next_index = np.argmax(np.random.multinomial(1, exp_preds / np.sum(exp_preds), 1))\n",
        "        next_char = char_from_index[next_index]\n",
        "\n",
        "        output = output[1:] + next_char\n",
        "\n",
        "        print(next_char, end = '')\n",
        "    print()\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "# Save the model\n",
        "checkpoint = ModelCheckpoint('Models/model-epoch-{epoch:02d}.hdf5', \n",
        "                             monitor = 'loss', verbose = 1, save_best_only = True, mode = 'min')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Uayl4WZOjpz"
      },
      "source": [
        "The code below will start the model to train. This may take a long time. Feel free to stop the training with the `square stop button` to the right of the `Run button` in the toolbar.\n",
        "\n",
        "Later in the exercise, we will load a pretrained model.\n",
        "\n",
        "### In the cell below replace:\n",
        "#### 1. `<addPrintCallback>` with `print_callback`\n",
        "#### 2. `<addCheckpoint>` with `checkpoint`\n",
        "#### and then __run the code__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "cUrTPcCPOjpz",
        "outputId": "c074adb1-0b36-4561-ab3c-f91c7af45c2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        }
      },
      "source": [
        "###\n",
        "# REPLACE <addPrintCallback> WITH print_callback AND <addCheckpoint> WITH checkpoint\n",
        "###\n",
        "model.fit(X, y, batch_size = 128, epochs = 3, callbacks = [print_callback, checkpoint])\n",
        "###"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "340/341 [============================>.] - ETA: 0s - loss: 2.7411\n",
            "### Generating text with diversity 0.50\n",
            "### Generating with seed: \" i had nothing left but misery then i sl\"\n",
            " i had nothing left but misery then i sle rog iord tre tse the  ahe an s tit ra at t the the the the ber on  one leras uofed ia tteis the inr ar the the al  ar the al sore the go ome snd in thare mre ind re in  renl panatir the rit ptoe the ther oo  ro tro t ole ter re lons and mee ore on tor as de pe tan  iunr  an  i rovcim on oola aghin  r the lh than i i ao me othe  ort re re in se iwe ah the te tog ind ine m n tes lo le tore se the thane i  fe mov tht mous thel in cy re te tidd an er no i y en mi e th on feti ne as an ahe  at on o\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.74094, saving model to Models/model-epoch-01.hdf5\n",
            "341/341 [==============================] - 58s 170ms/step - loss: 2.7409\n",
            "Epoch 2/3\n",
            "340/341 [============================>.] - ETA: 0s - loss: 2.3368\n",
            "### Generating text with diversity 0.50\n",
            "### Generating with seed: \"her and waiting for the dark in my excit\"\n",
            "her and waiting for the dark in my excithe the waced he the too the tht in the mor ing in binet i the the the fod ti to the the thins and in the thi the an sintid the tha the the the the cane the inr the the bigis pareis in dot ithe the salo the the sat the s in to e ninhe d in the the the ing on the sant in the son core the the the whel then th the the the sos the thi in thicg in the the the the io thof the was  and and wathe aroe the tore the mand in the the sere in the toar iut in in wa the the the bithe s ane sos the don shas the \n",
            "\n",
            "Epoch 00002: loss improved from 2.74094 to 2.33656, saving model to Models/model-epoch-02.hdf5\n",
            "341/341 [==============================] - 59s 172ms/step - loss: 2.3366\n",
            "Epoch 3/3\n",
            "340/341 [============================>.] - ETA: 0s - loss: 2.2058\n",
            "### Generating text with diversity 0.50\n",
            "### Generating with seed: \" a precious poor dream at times—but i ca\"\n",
            " a precious poor dream at times—but i care bube ti he the the be sis ba the ther the the the ware the the the ar ind watt a mere the red wat and the the the wat ou has and the morend ind moche thit as the sasthe th the sering in the ald a peand the wer of the wall and ant o the in the the wat in the lercad an the the was of and and ar tous and thele stoun and in ar and the the in the the the the the sire the beli the the the mot and reas in the surat i dati o seon was was lithin of the the wand the mece and sof ha sar of at in in the \n",
            "\n",
            "Epoch 00003: loss improved from 2.33656 to 2.20561, saving model to Models/model-epoch-03.hdf5\n",
            "341/341 [==============================] - 59s 173ms/step - loss: 2.2056\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0409bbb6a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02xAZDv8Ojp2"
      },
      "source": [
        "The output won't appear to be very good. But then, this dataset is small, and we have trained it only for a short time using a rather small RNN. How might it look if we upscaled things?\n",
        "\n",
        "Step 5\n",
        "------\n",
        "\n",
        "We could improve our model by:\n",
        "* Having a larger training set.\n",
        "* Increasing the number of LSTM units.\n",
        "* Training it for longer\n",
        "* Experimenting with difference activation functions, optimization functions etc\n",
        "\n",
        "Training this would still take far too long on most computers to see good results - so we've trained a model already for you.\n",
        "\n",
        "This model uses a different dataset - a few of the King Arthur tales pasted together. The model used:\n",
        "* sequences of 50 characters\n",
        "* Two LSTM layers (512 units each)\n",
        "* A dropout of 0.5 after each LSTM layer\n",
        "* Only 30 epochs (we'd recomend 100-200)\n",
        "\n",
        "Let's try importing this model that has already been trained.\n",
        "\n",
        "#### Replace `<addLoadModel>` with `load_model` and run the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "c95JGRnvOjp3",
        "outputId": "c87917e8-16ff-417d-f1f1-d476cfc82f4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "print(\"loading model... \", end = '')\n",
        "\n",
        "###\n",
        "# REPLACE <addLoadModel> BELOW WITH load_model\n",
        "###\n",
        "model = load_model('Models/arthur-model-epoch-30.hdf5')\n",
        "###\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam')\n",
        "###\n",
        "\n",
        "print(\"model loaded\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading model... "
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-766ee7829df3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# REPLACE <addLoadModel> BELOW WITH load_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Models/arthur-model-epoch-30.hdf5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Adam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m       \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    111\u001b[0m                   (export_dir,\n\u001b[1;32m    112\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: Models/arthur-model-epoch-30.hdf5/{saved_model.pbtxt|saved_model.pb}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPcfDRjPOjp6"
      },
      "source": [
        "Step 6\n",
        "-------\n",
        "\n",
        "Now let's use this model to generate some new text!\n",
        "\n",
        "#### Replace `<addFilePath>` with `'Data/Arthur tales.txt'`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "BSHYIKWsOjp6",
        "outputId": "02a0325f-efba-4cb8-8536-af871c699f15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "###\n",
        "# REPLACE <addFilePath> BELOW WITH 'Data/Arthur tales.txt' (INCLUDING THE QUOTATION MARKS)\n",
        "###\n",
        "text = io.open('/content/The Time Machine.txt', encoding='UTF-8').read()\n",
        "###\n",
        "\n",
        "# Cut out punctuation and make lower case\n",
        "text = text.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "# Character index dictionary\n",
        "charset = sorted(list(set(text)))\n",
        "index_from_char = dict((c, i) for i, c in enumerate(charset))\n",
        "char_from_index = dict((i, c) for i, c in enumerate(charset))\n",
        "\n",
        "print('text length: %s characters' %len(text))\n",
        "print('unique characters: %s' %len(charset))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text length: 174201 characters\n",
            "unique characters: 39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf6bfWx7Ojp9"
      },
      "source": [
        "### In the cell below replace:\n",
        "#### 1. `<sequenceLength>` with `50`\n",
        "#### 2. `<writeSentence>` with a sentence of your own, at least 50 characters long.\n",
        "#### 3. `<numCharsToGenerate>` with the number of characters you want to generate (choose a large number, like 1500)\n",
        "#### and then __run the code__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "6pINI078Ojp-",
        "outputId": "c4b5ee83-3731-43ff-8b15-0437235e3271",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "# Generate text\n",
        "\n",
        "diversity = 0.5\n",
        "print('\\n### Generating text with diversity %0.2f' %(diversity))\n",
        "\n",
        "###\n",
        "# REPLACE <sequenceLength> BELOW WITH 50\n",
        "###\n",
        "sequence_length = 50\n",
        "###\n",
        "\n",
        "# Next we'll make a starting point for our text generator\n",
        "\n",
        "###\n",
        "# REPLACE <writeSentence> WITH A SENTENCE OF AT LEAST 50 CHARACTERS\n",
        "###\n",
        "seed = \"IT befell in the days of Uther Pendragon, when he was king of all England, and so reigned, that there was a mighty duke in Cornwall that held war against him long time. And the duke was called the Duke of Tintagil.\"\n",
        "###\n",
        "\n",
        "seed = seed.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "###\n",
        "# OR, ALTERNATIVELY, UNCOMMENT THE FOLLOWING TWO LINES AND GRAB A RANDOM STRING FROM THE TEXT FILE\n",
        "###\n",
        "\n",
        "#start = random.randint(0, len(text) - sequence_length - 1)\n",
        "#seed = text[start: start + sequence_length]\n",
        "\n",
        "###\n",
        "\n",
        "print('### Generating with seed: \"%s\"' %seed[:40])\n",
        "\n",
        "output = seed[:sequence_length].lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "print(output, end = '')\n",
        "\n",
        "###\n",
        "# REPLACE THE <numCharsToGenerate> BELOW WITH THE NUMBER OF CHARACTERS WE WISH TO GENERATE, e.g. 1500\n",
        "###\n",
        "for i in range(1500):\n",
        "###\n",
        "    x_pred = np.zeros((1, sequence_length, len(charset)))\n",
        "    for t, char in enumerate(output):\n",
        "        x_pred[0, t, index_from_char[char]] = 1.\n",
        "\n",
        "    predictions = model.predict(x_pred, verbose=0)[0]\n",
        "    exp_preds = np.exp(np.log(np.asarray(predictions).astype('float64')) / diversity)\n",
        "    next_index = np.argmax(np.random.multinomial(1, exp_preds / np.sum(exp_preds), 1))\n",
        "    next_char = char_from_index[next_index]\n",
        "\n",
        "    output = output[1:] + next_char\n",
        "\n",
        "    print(next_char, end = '')\n",
        "print()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "### Generating text with diversity 0.50\n",
            "### Generating with seed: \"it befell in the days of uther pendragon\"\n",
            "it befell in the days of uther pendragon when he wat of in the wand and and alden a the the sas at and and the surtor in the wat the the she the ware the ther rast sured and in the to the ther and ande and stoled and and on hat in stale and ware toor a that whing an the the war the in sorer a se werere a sorere in in and ald the tha le the sed and in the han me wir the ther are as a the was and tool sor aad in the the fore noun and and an the waad the of and and of ther and er in the the sof ind in and the whe w or the the the whi ther in the wat my and of ther and and wa faon the the laed thon the ghes or shas ha the the the the the bore he rere d ard an thi hat on and ther and the thar the fathe das ind lere the the mar the the the the the the the the steas in the the the serrer in the tore the wit ind core tha s our ia the mere thare oa the the theat of the seron the cathe thi the war in the cats thi bith the thit in the sere buth me ind the the the the s and ind bealot war ey and of sere cale sanit ou and the me the dere ou the the and and aad in the tili in the wime was ha wat we tho kis the blat in s ow ay mout se and or an boul was and ther wast of the tho le the in the the ware mat ther thithe had the t om the the the stare se anding an the the wath of wad in the the the ther ald the the the wer and the ther ald and laming to the with the the they the thas ur and tha mit the the the the the the sower and on the the the thed the ter an the the the that of or a mape and the the care and s and and was the wet ferererith\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEVQTrroOjqA"
      },
      "source": [
        "How does it look? Does it seem intelligible?\n",
        "\n",
        "Conclusion\n",
        "--------\n",
        "\n",
        "We have trained an RNN that learns to predict characters based on a text sequence. We have trained a lightweight model from scratch, as well as imported a pre-trained model and generated new text from that."
      ]
    }
  ]
}